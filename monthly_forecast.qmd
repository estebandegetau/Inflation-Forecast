---
title: "Pronóstico de inflación en México"
subtitle: "Un enfoque de aprendizaje automático"
author: 
  - name: Esteban Degetau
    email: esteban.degetau@imss.gob.mx
    orcid: 0009-0004-4095-8819
    organization: Instituto Mexicano del Seguro Social
format: 
  html:
    toc: true
    theme: 
      light: flatly
      dark: darkly
    code-links: 
      - text: Código fuente
        href: https://github.com/estebandegetau/Inflation-Forecast
        icon: github
    other-links:
      - text: Pronósticos
        href: data/forecasts.csv
        icon: download
      - text: Pruebas
        href: data/tests.csv
        icon: download
      - text: Encuesta Banxico
        href: data/banxico_survey.csv
        icon: download
    fig-responsive: true
    fig-dpi: 500
    fig-width: 9
    fig-asp: 0.6
  # pdf:
  #   include-in-header:
  #     - text: |
  #         \usepackage[dvipsnames]{xcolor}    
  docx: default
execute:
    echo: false
    warning: false
    freeze: auto
    cache: true
lang: es
toc: false
bibliography: references.bib
nocite: |
    @allen2022; @hyndman2021; @hamming1997; @hyndman2008; @banxico_sie
---

```{r}
#| label: setup
#| include: false
#| cache: false

rm(list = ls())
gc()

pacman::p_load(
    tidyverse,
    here,
    labelled,
    forecast,
    gt,
    gtsummary,
    corrplot,
    imputeTS,
    olsrr,
    plotly,
    webshot2
)



theme_set(theme_minimal())

```

```{r}
#| label: load-data
#| include: false
#| cache: false

load(here::here("data/inputs.RData"))

updated_month <- updated_data |> month()

month_day <- today() |> day()
month <- today() |> month()

if(month > updated_month & month_day >= 9) {

    source(here::here("R/00_run.R"))

    load(here::here("data/inputs.RData"))

    theme_set(theme_minimal())

    # Clear cache
    file.remove(here::here("monthly_forecast_cache"))


} 

load(here("data/models.RData"))

last_obs <- inputs |>
    drop_na(infl_mensual) |>
    pull(fecha) |>
    max()

train_months <- timeline |>
  filter(kind == "Train") |>
  pull(fecha)

test_months <- timeline |>
  filter(kind == "Test") |>
  pull(fecha)

predict_months <- timeline |>
  filter(str_detect(kind, "Predict")) |>
  pull(fecha)
 
h <- length(predict_months)

```

```{r}
#| label: models
#| include: false



model_names <- models |> pull(name) |>
    str_c(collapse = ", ") |>
    # Replace last ", " with ", y "
    str_replace_all(", (?!.*,)", ", y ") |>
    str_glue()

```

```{r functions}

format_date <- function(date) {
  
  
  date |>
    as_date() |>
    format("%B %Y")
  
  
  
}

```

::: callout-note
Este reporte se actualizó por última vez el `r updated_data |> format("%d %B %Y a las %H:%M hrs CDMX")`.
:::

## Modelos univariados

Hay al menos 50 maneras distintas de predecir el futuro, lo que implica que todavía no tenemos una herramienta de predicción perfecta [@hamming1997]. Una manera de generar predicciones automáticamente es con modelos univariados, que solo utilizan una serie de tiempo como insumo de sus predicciones [@hyndman2008].

En esta sección, comparamos los modelos `r model_names` para predecir la inflación mensual en México.[^1] De estos, el modelo ARIMA es el más conocido y utilizado en la literatura. Los modelos ETS y LM son útiles por ser más simples. Por último, el modelo NNETAR es un modelo de redes neuronales para series de tiempo que ha demostrado ser útil en la predicción de series de tiempo [@allen2022].

### Probando los modelos

Una manera de comparar distintos modelos de predicción es evaluar su precisión en un conjunto de datos externo; es decir, en un conjunto de datos que no se utilizó para ajustar los modelos [@hyndman2021; @james2021a]. En este caso, utilizamos los datos de inflación mensual desde `r min(train_months) |> format_date()` hasta `r max(train_months) |> format_date()` para ajustar los modelos y los datos desde `r min(test_months) |> format_date()` hasta `r max(test_months) |> format_date()` para evaluar su precisión. La @fig-timeline muestra la serie de tiempo de inflación mensual (mes con mes) y los periodos de entrenamiento y prueba.

```{r}
#| label: fig-timeline
#| fig-cap: Línea de tiempo de entrenamiento, prueba y pronóstico
#| fig-column: page
#| fig-width: 12
#| fig-height: 6

a <- inputs |>
    left_join(timeline) |>
    filter(kind != "Predict Ex-Ante") |>
    mutate(
        kind = case_when(
            kind == "Train" ~ "Entrenamiento",
            kind == "Test" ~ "Prueba"
        ),
        kind = factor(kind, levels = c("Prueba", "Entrenamiento"))
    ) |>
    ggplot(aes(x = fecha, y = infl_mensual, color = kind)) +
    geom_line() +
    labs(
        x = "",
        y = "Inflación mensual (%)",
        color = ""
    ) +
    scale_color_brewer(palette = "Set1") +
    scale_x_date(date_labels = "%Y", date_breaks = "5 year") +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = test_months[1], linetype = "dashed") +
    coord_cartesian(ylim = c(-1, 2)) +
    theme(legend.position = "bottom")
  
a

```

La @fig-test muestra las predicciones de los modelos en el conjunto de prueba contra la inflación mensual observada. El modelo NNETAR parece ser el más preciso.

```{r}
#| label: fig-test
#| fig-cap: Prueba de los modelos


test_forecasts <- models |>
  mutate(
    forecast = map(forecast, pluck, "mean"),
    fecha = list(test_months |> as_date())
  ) |>
  select(name, forecast, fecha) |>
  unnest(cols = !name) 

a <- ggplot() +
  geom_line(
    data = test_forecasts,
    aes(x = fecha, y = forecast, color = name),
  ) +
  geom_line(
    data = inputs |> filter(fecha %in% test_months),
    aes(x = fecha, y = infl_mensual, color = "Observado"),
  ) +
  geom_hline(yintercept = 0) +
  labs(
    x = "Período de prueba",
    y = "Inflación mensual (%)",
    color = ""
  ) +
  scale_color_viridis_d() +
  scale_x_date(date_labels = "%b %Y", date_breaks = "4 month")

a

```

```{r}
#| label: tbl-comp
#| tbl-cap: "Precisión de los modelos"
#| tbl-subcap:
#|  - Muestra de entrenamiento
#|  - Muestra de prueba
#| layout-ncol: 2


outside <- models |>
    select(Modelo = name, test) |>
    unnest(test)


inside <- models |>
  select(name, inside_accuracy) |>
  mutate(inside_accuracy = map(inside_accuracy, as_tibble)) |>
  select(Modelo = name, inside_accuracy) |>
  unnest(inside_accuracy) 
  
  
best_inside <- inside |>
    filter(RMSE == min(RMSE)) |>
    pull(Modelo)



best_outside <- outside |>
    filter(RMSE == min(RMSE)) |>
    pull(Modelo)


final_model <- models |> filter(name == best_outside) 


inside |>
  select(1:5) |>
  gt() |>
  fmt_number(
      columns = 2:5,
      decimals = 2
  ) 


outside |>
    gt() |>
    fmt_number(
        columns = 2:5,
        decimals = 2
    )




```

La @tbl-comp muestra algunas métricas de precisión de los pronósticos dentro y fuera de la muestra de entrenamiento.[^2] El modelo más preciso fue `r best_outside`, puesto que tuvo errores absolutos y cuadráticos más pequeños fuera de la muestra. Estas métricas son útiles para comparar diferentes modelos y las seguiremos utilizando para evaluar la precisión de los modelos en el pronóstico ex-ante.

### Pronóstico ex-ante

Sabiendo cuán precisos son los modelos, procedo a pronosticar la inflación mensual en México desde `r min(predict_months) |> format_date()` hasta `r max(predict_months) |> format_date()`. Solo para fines comparativos, utilizaré todos los modelos aunque el mejor fue indiscutiblemente `r best_outside`.

La @fig-forecast muestra los pronósticos de los modelos para los siguientes `r h` meses. Algo importante para notar es que el modelo NNETAR (@fig-forecast-4) no produce intervalos de confianza, puesto que las predicciones por redes neuronales pierden interpretabilidad y no permiten tener una medida de la varianza de la predicción.

```{r}
#| label: fig-forecast
#| fig-cap: Pronósticos ex-ante de inflación mensual
#| fig-subcap: 
#|   - ARIMA
#|   - ETS
#|   - LM
#|   - NNETAR
#| layout-ncol: 2
#| fig-width: 5
#| fig-asp: 0.8

plot_forecast <- function(forecast) {
  
  forecast |>
    autoplot(
      xlab = "",
      ylab = "Inflación mensual (%)"
    ) +
    geom_hline(yintercept = 0, color = "grey") +
    coord_cartesian(xlim = c(2022, 2026), ylim = c(-3, 4))
  
  
}

plots <- models |>
  mutate(
    plot = map(full_forecast, plot_forecast)
  )

plots |> pull(plot) |> walk(print)




```

```{r load-banxico}

load(here::here("data/fcast_svy.RData"))

mean_fcast <- fcast_svy |>
    filter(stat == "Media", month %in% predict_months) 


```

Una manera de evaluar la precisión de los pronósticos ex-ante es con respecto a la encuesta de expectativas de inflación mensual del Banco de México [@banxico_sie]. La @fig-banxico muestra los pronósticos de los modelos para los siguientes 12 meses y los compara con la media de las respuestas de la encuesta. Ninguno de los modelos que entrené se ajusta perfectamente al promedio de las expectativas, pero NNETAR y ETS están muy cerca.

```{r}
#| label: fig-banxico
#| fig-cap: Pronóstico de inflación mensual a doce meses vs media de encuesta de expectativas Banxico

final_forecasts <- models |>
  mutate(
    full_forecast = map(full_forecast, pluck, "mean"),
    fecha = list(predict_months)
  ) |>
  select(name, full_forecast, fecha) |>
  unnest(cols = !name) 

a <- ggplot() +
  geom_line(
    data = final_forecasts |> filter(fecha %in% predict_months[1:12]),
    aes(x = fecha, y = full_forecast, color = name),
  ) +
  geom_line(
    data = mean_fcast,
    aes(x = month, y = value, color = "Promedio analistas"),
  ) +
  geom_hline(yintercept = 0) +
  labs(
    x = "",
    y = "Inflación mensual (%)",
    color = ""
  ) +
  scale_color_viridis_d() 

a 

```

La @tbl-banxico muestra la precisión de los modelos en la muestra completa respecto a la media de las expectativas de los analistas de Banxico. Es importante notar que el mejor modelo en nuestras pruebas (NNETAR) no es el que resultó estar más cerca del promedio de los analistas. Sin embargo, esto obliga hacer la pregunta ¿qué tan preciso puede ser el proimedio de analistas para predecir la inflación? La @sec-survey busca responder esta pregunta.

Otra manera de medir la precisión del pronóstico es comparando la inflaicón anual para 2025 con la media de las expectativas de los analistas de Banxico. La @tbl-yearly muestra que el modelo NNETAR tuvo una predicción muy cercana a la expectativa promedio de los analistas de Banxico.

```{r}
#| label: tbl-banxico
#| tbl-cap: Precisión de los modelos en la muestra completa respecto la media de analistas Banxico


test_banxico <- function(forecast) {
  forecast <- forecast |>
    pluck("mean") |>
    as.numeric()
  forecast <- forecast[1:12]
  
  analysts_mean <- mean_fcast$value[1:12] |> as.numeric()
  
  error <- forecast - analysts_mean
  
  mae <- mean(abs(error))
  rmse <- sqrt(mean(error ^ 2))
  me <- mean(error)
  mpe <- 100 * mean(error / analysts_mean)
  
  tibble(
    ME = me,
    RMSE = rmse,
    MAE = mae,
    MPE = mpe
  )
  
  
}

models |>
  mutate(
    banxico_test = map(full_forecast, test_banxico)
  ) |>
  select(Modelo = name, banxico_test) |>
  unnest(banxico_test) |>
  gt() |>
  fmt_number(
    columns = 2:5,
    decimals = 2
  )
  

```

```{r}
#| label: tbl-yearly
#| tbl-cap: Precisión de los pronósticos para inflación anual 2025 respecto a encuesta de expectativas Banxico

load(here("data/yearly_survey.RData"))

yearly_svey <- yearly_svey |>
    filter(year == 2025)

yearly_comp <- final_forecasts |>
    mutate(year = year(fecha)) |>
    filter(year == 2025) |>
    group_by(name, year) |>
    summarise(
        forecast = sum(full_forecast)
    ) |>
    ungroup() |>
    full_join(yearly_svey, by = "year", relationship = "many-to-many") |>
    filter(str_detect(stat, "Media|Mediana|cuartil|imo")) |>
    mutate(
        stat = factor(stat, levels = c(
            "Mínimo",
            "Primer cuartil",
            "Media",
            "Mediana",
            "Tercer cuartil",
            "Máximo"
        ))
    ) |>
    mutate(
        Pronóstico = forecast,
        Banxico = value,
        Error = forecast - value,
        `Error porcentual` = 100 * ((forecast - value) / value),
        # RSE = sqrt((Error^2)),
        # AE = abs(Error)
    ) |>
    pivot_longer(
        c(
          Pronóstico,
          Banxico,
          Error, 
        `Error porcentual`
        ),
        names_to = "measure",
        values_to = "measure_value"
    )

measures <- yearly_comp |> pull(measure) |> unique()


tab_errors <- function(measure) {

    a <- yearly_comp |>
        filter(measure == !!measure) |>
        select(Modelo = name, stat, measure_value) |>
        pivot_wider(
            names_from = stat,
            values_from = measure_value
        ) |>
        gt() |>
        fmt_number(
            columns = 2:7,
            decimals = 2
        )
        
    return(a)      


}

yearly_comp |>
  filter(stat == "Media") |>
  select(Modelo = name, measure, value = measure_value) |>
  pivot_wider(
    names_from = measure,
    values_from = value
  ) |>
  gt() |>
  fmt_number(
    columns = 2:5,
    decimals = 2
  )

```

## Poder predictivo de la encuesta de expectativas {#sec-survey}

Esta sección evalúa la capacidad predictiva de la encuesta de expectativas de inflación del Banco de México.

La @fig-expectations muestra que las predicciones intercuartílicas (primer cuatil, media, mediana, y tercer cuartil) tienen aproximadamente la misma capacidad predictiva a lo largo de todo el horizonte de predicción (de cero a doce meses de anticipación). Es importante destacar que los errores absolutos (MAE y RMSE) son no monotónicos; es decir,. las mejores predicciones son con anticipaciones cercanas a cero y a doce meses. Las peores predicciones son a seis meses de anticipación.

```{r}
#| label: fig-expectations
#| fig-cap: Precisión de la encuesta de expectativas Banxico

load(here::here("data", "survey_history.RData"))

inf <- inputs |> select(reference_date = fecha, infl_mensual)

survey_comp <- survey_history |>
    mutate(
        fecha = as_date(reference_date)
    ) |>
    left_join(inf) |>
    mutate(
        error = value - infl_mensual,
        p_error = 100 * ((value - infl_mensual) / infl_mensual),
        t = factor(t)
    ) |>
    drop_na(value, infl_mensual) 

survey_summary <- survey_comp |>
    group_by(t, stat) |>
    summarise(
        MAE = mean(abs(error)),
        RMSE = sqrt(mean(error^2)),
        ME = mean(error),
        MPE = mean(p_error)
    ) |>
    ungroup() |>
    pivot_longer(
        where(is.numeric),
        names_to = "measure"
    ) |>
    filter(str_detect(stat, "Media|Mediana|cuartil|imo")) |>
    mutate(
        stat = factor(stat, levels = c(
            "Mínimo",
            "Primer cuartil",
            "Media",
            "Mediana",
            "Tercer cuartil",
            "Máximo"
        )),
        measure = factor(measure, levels = c(
            "MAE",
            "RMSE",
            "ME",
            "MPE"
        ))
    ) 

a <- survey_summary |>
    ggplot(aes(x = as.numeric(t) - 1, y = value, color = stat)) +
    geom_line() +
    geom_hline(yintercept = 0, color = "black") +
    facet_wrap(~measure, scales = "free_y") +
    scale_x_continuous(breaks = seq(0, 12, 2)) +
    labs(
      x = "Meses de anticipación",
      y = "",
      color = "Encuesta Banxico"
    ) +
    scale_color_viridis_d(option = "D")

a


```

La @tbl-expectations muestra que las predicciones intercuartílicas tienen un error cuadrático histórico de 0.41 puntos porcentuales. Al comparar con los errores de mis modelos en la @tbl-comp-2, el modelo NNETAR tuvo mejor desempeño que el histórico de los analistas.

```{r}
#| label: tbl-expectations
#| tbl-cap: Precisión histórica de la encuesta de expectativas Banxico

hist_svy_acc <- survey_comp |>
    group_by(stat) |>
    summarise(
        MAE = mean(abs(error)),
        RMSE = sqrt(mean(error^2)),
        ME = mean(error),
        MPE = mean(p_error)
    ) |>
    ungroup() |>
    pivot_longer(
        where(is.numeric),
        names_to = "measure"
    ) |>
    filter(str_detect(stat, "Media|Mediana|cuartil|imo")) |>
    mutate(
        stat = factor(stat, levels = c(
            "Mínimo",
            "Primer cuartil",
            "Media",
            "Mediana",
            "Tercer cuartil",
            "Máximo"
        )),
        measure = factor(measure, levels = c(
            "MAE",
            "RMSE",
            "ME",
            "MPE"
        ))
    ) |>
  
  pivot_wider(
    names_from = measure
  ) |>
  arrange(stat) 

hist_svy_acc |>
  rename(
    "Estadístico" = stat
  ) |>
  gt(  ) |>
  fmt_number(
    columns = 2:5,
    decimals = 2
  )

```


## Conclusión

```{r}

best_rmse <- models |>
    filter(name == best_outside) |>
    pull(test) |>
    unlist() |>
    pluck("RMSE")

svy_rmse <- hist_svy_acc |>
  filter(stat == "Media") |>
  pull(RMSE) 

```

El mejor de los modelos que entrené resultó ser más preciso que la expectativa media de los analistas encuestados por Banco de México históricamente. En particular, el modelo `r best_outside` tuvo un error cuadrático medio de `r best_rmse |> round(2)` en el periodo de prueba, mientras que la el promedio de los analistas tuvo un error cuadrático medio histórico de `r svy_rmse |> round(2)` puntos porcentuales. 

## Referencias

[^1]: ARIMA: Modelo autorregresivo integrado de media móvil.

    ETS: Suavización exponencial

    LM: Modelo lineal

    NNETAR: Redes neuronales para series de tiempo

[^2]: ME: *Median Error.-* Error de estimación promedio.

    RMSE: *Root median squared error.-* Raíz cuadrada del error cuadrático medio.

    MAE: *Mean absolute error.-* Error absoluto promedio.

    MPE: *Mean percentage error.-* Error porcentual promedio.